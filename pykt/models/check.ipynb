{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.init import constant_\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from enum import IntEnum\n",
    "import numpy as np\n",
    "# from .utils import transformer_FFN, ut_mask, pos_encode, get_clones\n",
    "from torch.nn import Module, Embedding, LSTM, Linear, Dropout, LayerNorm, TransformerEncoder, TransformerEncoderLayer, \\\n",
    "        MultiLabelMarginLoss, MultiLabelSoftMarginLoss, CrossEntropyLoss, BCELoss, MultiheadAttention\n",
    "from torch.nn.functional import one_hot, cross_entropy, multilabel_margin_loss, binary_cross_entropy\n",
    "# from .que_base_model import QueBaseModel,QueEmb\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    "\n",
    "class UNIKT(nn.Module):\n",
    "    def __init__(self, n_question=100000, n_pid=188, \n",
    "            d_model=256, n_blocks=5, dropout=0.1, d_ff=256, \n",
    "            loss1=0.5, loss2=0.5, loss3=0.5, start=50, num_layers=2, nheads=4, seq_len=1024, \n",
    "            kq_same=1, final_fc_dim=512, final_fc_dim2=256, num_attn_heads=8, separate_qa=False, l2=1e-5, emb_type=\"qid\", emb_path=\"\", pretrain_dim=768, cf_weight=0.3, t_weight=0.3, local_rank=1, num_sgap=None, c0=0, max_epoch=0, q_window_size=20, c_window_size=4):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            d_model: dimension of attention block\n",
    "            final_fc_dim: dimension of final fully connected net before prediction\n",
    "            num_attn_heads: number of heads in multi-headed attention\n",
    "            d_ff : dimension for fully conntected net inside the basic block\n",
    "            kq_same: if key query same, kq_same=1, else = 0\n",
    "        \"\"\"\n",
    "        self.model_name = \"unikt\"\n",
    "        print(f\"model_name: {self.model_name}, emb_type: {emb_type}\")\n",
    "        self.n_question = n_question\n",
    "        self.dropout = dropout\n",
    "        self.kq_same = kq_same\n",
    "        self.n_pid = n_pid\n",
    "        self.l2 = l2\n",
    "        self.model_type = self.model_name\n",
    "        self.separate_qa = separate_qa\n",
    "        self.emb_type = emb_type\n",
    "        self.ce_loss = BCELoss()\n",
    "        self.cf_weight = cf_weight\n",
    "        self.t_weight = t_weight\n",
    "        self.num_sgap = num_sgap\n",
    "        self.q_window_size = q_window_size\n",
    "        self.c_window_size = c_window_size\n",
    "        # print(f\"q_window_size:{self.q_window_size}\")\n",
    "        # print(f\"c_window_size:{self.c_window_size}\")\n",
    "\n",
    "        \n",
    "        self.embed_l = d_model\n",
    "\n",
    "        self.dataset_emb = nn.Embedding(20,self.embed_l).to(device)# dataset_id embedding\n",
    "\n",
    "        self.qa_embed = nn.Embedding(2, self.embed_l)\n",
    "        \n",
    "        if self.emb_type.find(\"pt\") != -1:\n",
    "            self.time_emb = nn.Embedding(self.num_sgap+1, self.embed_l)\n",
    "        # Architecture Object. It contains stack of attention block\n",
    "\n",
    "        # self.emb_q = nn.Sequential(\n",
    "        #     nn.Linear(1, 200000), nn.ReLU(), nn.Dropout(self.dropout),\n",
    "        #     nn.Linear(200000,d_model)\n",
    "        # )\n",
    "\n",
    "        self.emb_q = nn.Embedding(200000,self.embed_l).to(device)# question embedding\n",
    "\n",
    "        \n",
    "        # self.emb_c = nn.Sequential(\n",
    "        #     nn.Linear(7,1000), nn.ReLU(), nn.Dropout(self.dropout), # 7表示一个问题最大的concept数量\n",
    "        #     nn.Linear(1000,d_model)\n",
    "        # )\n",
    "\n",
    "        self.emb_c = nn.Parameter(torch.randn(1000, self.embed_l).to(device), requires_grad=True)# kc embedding\n",
    "\n",
    "\n",
    "        self.model = Architecture(n_question=n_question, n_blocks=n_blocks, n_heads=num_attn_heads, dropout=dropout,\n",
    "                                    d_model=d_model, d_feature=d_model / num_attn_heads, d_ff=d_ff,  kq_same=self.kq_same, model_type=self.model_type, seq_len=seq_len)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(d_model + self.embed_l,\n",
    "                      final_fc_dim), nn.ReLU(), nn.Dropout(self.dropout),\n",
    "            nn.Linear(final_fc_dim, final_fc_dim2), nn.ReLU(\n",
    "            ), nn.Dropout(self.dropout),\n",
    "            nn.Linear(final_fc_dim2, 1)\n",
    "        )\n",
    "        if emb_type.find(\"predc\") != -1:\n",
    "            self.qclasifier = nn.Sequential(\n",
    "                nn.Linear(d_model + self.embed_l,\n",
    "                        final_fc_dim), nn.ReLU(), nn.Dropout(self.dropout),\n",
    "                nn.Linear(final_fc_dim, final_fc_dim2), nn.ReLU(\n",
    "                ), nn.Dropout(self.dropout),\n",
    "                nn.Linear(final_fc_dim2, self.n_pid)\n",
    "            )\n",
    "            \n",
    "        if emb_type.find(\"pt\") != -1: \n",
    "            self.t_out = nn.Sequential(\n",
    "                nn.Linear(d_model + self.embed_l,\n",
    "                        final_fc_dim), nn.ReLU(), nn.Dropout(self.dropout),\n",
    "                nn.Linear(final_fc_dim, final_fc_dim2), nn.ReLU(\n",
    "                ), nn.Dropout(self.dropout),\n",
    "                nn.Linear(final_fc_dim2, 1)\n",
    "            )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for p in self.parameters():\n",
    "            if p.size(0) == self.n_pid+1 and self.n_pid > 0:\n",
    "                torch.nn.init.constant_(p, 0.)\n",
    "\n",
    "    def get_avg_skill_emb(self,c):\n",
    "        # add zero for padding\n",
    "        concept_emb_cat = torch.cat(\n",
    "            [torch.zeros(1, self.embed_l).to(device), \n",
    "            self.emb_c], dim=0)\n",
    "        # shift c\n",
    "\n",
    "        related_concepts = (c+1).long()\n",
    "        #[batch_size, seq_len, emb_dim]\n",
    "        concept_emb_sum = concept_emb_cat[related_concepts, :].sum(\n",
    "            axis=-2)\n",
    "\n",
    "        #[batch_size, seq_len,1]\n",
    "        concept_num = torch.where(related_concepts != 0, 1, 0).sum(\n",
    "            axis=-1).unsqueeze(-1)\n",
    "        concept_num = torch.where(concept_num == 0, 1, concept_num)\n",
    "        concept_avg = (concept_emb_sum / concept_num)\n",
    "        return concept_avg\n",
    "\n",
    "    def forward(self, dcur, qtest=False, train=False, dgaps=None):\n",
    "        q, c, r = dcur[\"qseqs\"].long().to(device), dcur[\"cseqs\"].long().to(device), dcur[\"rseqs\"].long().to(device)\n",
    "        qshft, cshft, rshft = dcur[\"shft_qseqs\"].long().to(device), dcur[\"shft_cseqs\"].long().to(device), dcur[\"shft_rseqs\"].long().to(device)\n",
    "        # print(f\"q:{q.shape}\")\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        dataset_id = dcur[\"dataset_id\"].long().to(device)\n",
    "        pid_data = torch.cat((q[:,0:1], qshft), dim=1) # shape[batch,200]\n",
    "        q_data = torch.cat((c[:,0:1], cshft), dim=1) # shape[batch,200,7]\n",
    "        target = torch.cat((r[:,0:1], rshft), dim=1)\n",
    "\n",
    "        if self.emb_type.find(\"aug\") != -1 and train:\n",
    "            # new qids\n",
    "            aug_pids= pid_data.unfold(1, self.q_window_size, 1)\n",
    "            aug_pids = aug_pids.sum(dim=2)\n",
    "            aug_pid_data = torch.where(aug_pids >= 200000, -1, aug_pids)\n",
    "\n",
    "            # new cids\n",
    "            if q_data.size(2) <  self.c_window_size:\n",
    "                aug_cids = q_data.unfold(2, q_data.size(2), 1).sum(dim=-1)\n",
    "            else:\n",
    "                aug_cids = q_data.unfold(2, self.c_window_size, 1).sum(dim=-1)\n",
    "            aug_cid_data = torch.where(aug_cids >= 1000, -1, aug_cids)\n",
    "            # print(f\"aug_cid_data:{aug_cid_data.shape}\")\n",
    "\n",
    "            # new rids\n",
    "            window_counts = target.unfold(1, self.q_window_size, 1).sum(dim=2)\n",
    "            aug_target_data = (window_counts > self.q_window_size / 2).float().long()\n",
    "\n",
    "            # padding\n",
    "            pad_dims = [0, pid_data.size(1) - aug_pid_data.size(1), 0, pid_data.size(0) - aug_pid_data.size(0)]\n",
    "            tmp_aug_pid_data_padded = F.pad(aug_pid_data, pad_dims, value=-1)\n",
    "            aug_pid_data_padded = torch.where(tmp_aug_pid_data_padded == -1, 0, tmp_aug_pid_data_padded)\n",
    "            # print(f\"aug_pid_data_padded:{aug_pid_data_padded.shape}\")\n",
    "            cid_padding_size = q_data.size(2) - aug_cid_data.size(2)\n",
    "            aug_cid_data_padded = F.pad(aug_cid_data, (0, cid_padding_size))\n",
    "            # print(f\"aug_cid_data_padded:{aug_cid_data_padded.shape}\")\n",
    "            aug_target_data = F.pad(aug_target_data, pad_dims, value=0)\n",
    "\n",
    "            # stack data\n",
    "            all_pid_data = torch.cat([pid_data, aug_pid_data_padded],dim=0)\n",
    "            # print(f\"all_pid_data:{torch.max(all_pid_data)}\")\n",
    "            all_cid_data = torch.cat([q_data, aug_cid_data_padded],dim=0)\n",
    "            # print(f\"all_cid_data:{torch.max(all_cid_data)}\")\n",
    "            all_target_data = torch.cat([target, aug_target_data],dim=0)\n",
    "            # print(f\"all_target_data:{torch.max(all_target_data)}\")\n",
    "\n",
    "            # embedding\n",
    "            emb_q = self.emb_q(all_pid_data) #[batch,max_len-1,emb_size]\n",
    "            emb_c = self.get_avg_skill_emb(all_cid_data) #[batch,max_len-1,emb_size]\n",
    "            dataset_embed_data = self.dataset_emb(dataset_id).unsqueeze(1).repeat(2,1,1)\n",
    "            qa_embed_data = self.qa_embed(all_target_data)\n",
    "        else:\n",
    "            emb_q = self.emb_q(pid_data)#[batch,max_len-1,emb_size]\n",
    "            emb_c = self.get_avg_skill_emb(q_data)#[batch,max_len-1,emb_size]\n",
    "            dataset_embed_data = self.dataset_emb(dataset_id).unsqueeze(1)\n",
    "            # print(f\"dataset_embed_data:{dataset_embed_data.shape}\")\n",
    "            try:\n",
    "                qa_embed_data = self.qa_embed(target)\n",
    "            except:\n",
    "                print(f\"target:{target}\")\n",
    "        \n",
    "        if self.emb_type.find(\"pt\") != -1:\n",
    "            sg, sgshft = dgaps[\"sgaps\"].long(), dgaps[\"shft_sgaps\"].long()\n",
    "            s_gaps = torch.cat((sg[:, 0:1], sgshft), dim=1)\n",
    "            emb_t = self.time_emb(s_gaps)\n",
    "            q_embed_data += emb_t\n",
    "        \n",
    "        # print(f\"emb_q:{emb_q.shape}\")\n",
    "        # print(f\"emb_c:{emb_c.shape}\")\n",
    "        # print(f\"dataset_embed_data:{dataset_embed_data.shape}\")\n",
    "        q_embed_data = emb_q + emb_c + dataset_embed_data\n",
    "        qa_embed_data = q_embed_data + qa_embed_data\n",
    "\n",
    "        # BS.seqlen,d_model\n",
    "        y2, y3 = 0, 0\n",
    "        d_output = self.model((q_embed_data, qa_embed_data))\n",
    "        concat_q = torch.cat([d_output, q_embed_data], dim=-1)\n",
    "        output = self.out(concat_q).squeeze(-1)\n",
    "        m = nn.Sigmoid()\n",
    "        preds = m(output)\n",
    "\n",
    "        cl_losses = 0\n",
    "        if self.emb_type.find(\"predc\") != -1 and train:\n",
    "            sm = dcur[\"smasks\"].long()\n",
    "            start = 0\n",
    "            cpreds = self.qclasifier(concat_q[:,start:,:])\n",
    "            # print(f\"cpreds:{cpreds.shape}\")\n",
    "            flag = sm[:,start:]==1\n",
    "            # print(f\"flag:{flag.shape}\")\n",
    "            # print(f\"cpreds:{cpreds[:,:-1,:][flag].shape}\")\n",
    "            # print(f\"qtag:{q[:,start:][flag].shape}\")\n",
    "            cl_loss = self.ce_loss(cpreds[:,:-1,:][flag], q[:,start:][flag])\n",
    "            cl_losses += self.cf_weight * cl_loss\n",
    "        \n",
    "        if self.emb_type.find(\"pt\") != -1 and train:\n",
    "            t_label= dgaps[\"shft_pretlabel\"].double()\n",
    "            t_combined = torch.cat((d_output, emb_t), -1)\n",
    "            t_output = self.t_out(t_combined).squeeze(-1)\n",
    "            t_pred = m(t_output)[:,1:]\n",
    "            # print(f\"t_pred:{t_pred}\")\n",
    "            sm = dcur[\"smasks\"]\n",
    "            ty = torch.masked_select(t_pred, sm)\n",
    "            # print(f\"min_y:{torch.min(ty)}\")\n",
    "            tt = torch.masked_select(t_label, sm)\n",
    "            # print(f\"min_t:{torch.min(tt)}\")\n",
    "            t_loss = binary_cross_entropy(ty.double(), tt.double())\n",
    "            # t_loss = mse_loss(ty.double(), tt.double())\n",
    "            # print(f\"t_loss:{t_loss}\")\n",
    "            cl_losses += self.t_weight * t_loss\n",
    "        if self.emb_type.find(\"aug\") != -1 and train:\n",
    "            # cal loss of augmented data\n",
    "            aug_preds = preds[batch_size:]\n",
    "            # 拆分预测结果\n",
    "            preds = preds[:batch_size]\n",
    "\n",
    "            mask_select = tmp_aug_pid_data_padded != -1\n",
    "            select_preds = aug_preds[mask_select]\n",
    "            select_targets = aug_target_data[mask_select]\n",
    "            # print(f\"select_preds:{torch.max(select_preds)}\")\n",
    "            # print(f\"select_targets:{torch.max(select_targets)}\")\n",
    "            cl_losses = self.ce_loss(select_preds, select_targets.float())\n",
    "            # print(f\"cl_losses:{cl_losses}\")\n",
    "  \n",
    "        if train:\n",
    "            if self.emb_type == \"qid\":\n",
    "                return preds, y2, y3\n",
    "            else:\n",
    "                return preds, y2, y3, cl_losses\n",
    "        else:\n",
    "            if qtest:\n",
    "                return preds, concat_q\n",
    "            else:\n",
    "                return preds\n",
    "\n",
    "class Architecture(nn.Module):\n",
    "    def __init__(self, n_question,  n_blocks, d_model, d_feature,\n",
    "                 d_ff, n_heads, dropout, kq_same, model_type, seq_len):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "            n_block : number of stacked blocks in the attention\n",
    "            d_model : dimension of attention input/output\n",
    "            d_feature : dimension of input in each of the multi-head attention part.\n",
    "            n_head : number of heads. n_heads*d_feature = d_model\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.model_type = model_type\n",
    "\n",
    "        if model_type in {'gpt4kt','unikt'}:\n",
    "            self.blocks_2 = nn.ModuleList([\n",
    "                TransformerLayer(d_model=d_model, d_feature=d_model // n_heads,\n",
    "                                 d_ff=d_ff, dropout=dropout, n_heads=n_heads, kq_same=kq_same)\n",
    "                for _ in range(n_blocks)\n",
    "            ])\n",
    "        self.position_emb = CosinePositionalEmbedding(d_model=self.d_model, max_len=seq_len)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # target shape  bs, seqlen\n",
    "        q_embed_data, qa_embed_data = inputs\n",
    "        seqlen, batch_size = q_embed_data.size(1), q_embed_data.size(0)\n",
    "\n",
    "        q_posemb = self.position_emb(q_embed_data)\n",
    "        q_embed_data = q_embed_data + q_posemb\n",
    "        qa_posemb = self.position_emb(qa_embed_data)\n",
    "        qa_embed_data = qa_embed_data + qa_posemb\n",
    "\n",
    "        qa_pos_embed = qa_embed_data\n",
    "        q_pos_embed = q_embed_data\n",
    "\n",
    "        y = qa_pos_embed\n",
    "        seqlen, batch_size = y.size(1), y.size(0)\n",
    "        x = q_pos_embed\n",
    "\n",
    "        # encoder\n",
    "        \n",
    "        for block in self.blocks_2:\n",
    "            # x.requires_grad_(True)\n",
    "            # y.requires_grad_(True)\n",
    "            # def run_block(mask, query, key, values, apply_pos):\n",
    "            #     return block(mask, query, key, values, apply_pos)\n",
    "            # x = checkpoint(run_block, mask, x, x, y, apply_pos)\n",
    "            \n",
    "            x = checkpoint(block, x, x, y)\n",
    "            # x = block(mask=0, query=x, key=x, values=y, apply_pos=True) # True: +FFN+残差+laynorm 非第一层与0~t-1的的q的attention, 对应图中Knowledge Retriever\n",
    "            # mask=0，不能看到当前的response, 在Knowledge Retrever的value全为0，因此，实现了第一题只有question信息，无qa信息的目的\n",
    "            # print(x[0,0,:])\n",
    "            # x = input_data[1]\n",
    "        return x\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_feature,\n",
    "                 d_ff, n_heads, dropout,  kq_same):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "            This is a Basic Block of Transformer paper. It containts one Multi-head attention object. Followed by layer norm and postion wise feedforward net and dropout layer.\n",
    "        \"\"\"\n",
    "        kq_same = kq_same == 1\n",
    "        # Multi-Head Attention Block\n",
    "        self.masked_attn_head = MultiHeadAttention(\n",
    "            d_model, d_feature, n_heads, dropout, kq_same=kq_same)\n",
    "\n",
    "        # Two layer norm layer and two droput layer\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, values):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            block : object of type BasicBlock(nn.Module). It contains masked_attn_head objects which is of type MultiHeadAttention(nn.Module).\n",
    "            mask : 0 means, it can peek only past values. 1 means, block can peek only current and pas values\n",
    "            query : Query. In transformer paper it is the input for both encoder and decoder\n",
    "            key : Keys. In transformer paper it is the input for both encoder and decoder\n",
    "            Values. In transformer paper it is the input for encoder and  encoded output for decoder (in masked attention part)\n",
    "\n",
    "        Output:\n",
    "            query: Input gets changed over the layer and returned.\n",
    "\n",
    "        \"\"\"\n",
    "        mask = 0\n",
    "        apply_pos=True\n",
    "        seqlen, batch_size = query.size(1), query.size(0)\n",
    "        nopeek_mask = np.triu(\n",
    "            np.ones((1, 1, seqlen, seqlen)), k=mask).astype('uint8')\n",
    "        src_mask = (torch.from_numpy(nopeek_mask) == 0).to(device)\n",
    "        if mask == 0:  # If 0, zero-padding is needed.\n",
    "            # Calls block.masked_attn_head.forward() method\n",
    "            query2 = self.masked_attn_head(\n",
    "                query, key, values, mask=src_mask, zero_pad=True) # 只能看到之前的信息，当前的信息也看不到，此时会把第一行score全置0，表示第一道题看不到历史的interaction信息，第一题attn之后，对应value全0\n",
    "        else:\n",
    "            # Calls block.masked_attn_head.forward() method\n",
    "            query2 = self.masked_attn_head(\n",
    "                query, key, values, mask=src_mask, zero_pad=False)\n",
    "\n",
    "        query = query + self.dropout1((query2)) # 残差1\n",
    "        query = self.layer_norm1(query) # layer norm\n",
    "        if apply_pos:\n",
    "            query2 = self.linear2(self.dropout( # FFN\n",
    "                self.activation(self.linear1(query))))\n",
    "            query = query + self.dropout2((query2)) # 残差\n",
    "            query = self.layer_norm2(query) # lay norm\n",
    "        return query\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout, kq_same, bias=True):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        It has projection layer for getting keys, queries and values. Followed by attention and a connected layer.\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_feature\n",
    "        self.h = n_heads\n",
    "        self.kq_same = kq_same\n",
    "\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        if kq_same is False:\n",
    "            self.q_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj_bias = bias\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        xavier_uniform_(self.k_linear.weight)\n",
    "        xavier_uniform_(self.v_linear.weight)\n",
    "        if self.kq_same is False:\n",
    "            xavier_uniform_(self.q_linear.weight)\n",
    "\n",
    "        if self.proj_bias:\n",
    "            constant_(self.k_linear.bias, 0.)\n",
    "            constant_(self.v_linear.bias, 0.)\n",
    "            if self.kq_same is False:\n",
    "                constant_(self.q_linear.bias, 0.)\n",
    "            constant_(self.out_proj.bias, 0.)\n",
    "\n",
    "    def forward(self, q, k, v, mask, zero_pad):\n",
    "\n",
    "        bs = q.size(0)\n",
    "\n",
    "        # perform linear operation and split into h heads\n",
    "\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        if self.kq_same is False:\n",
    "            q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        else:\n",
    "            q = self.k_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k,\n",
    "                           mask, self.dropout, zero_pad)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous()\\\n",
    "            .view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out_proj(concat)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def attention(q, k, v, d_k, mask, dropout, zero_pad):\n",
    "    \"\"\"\n",
    "    This is called by Multi-head atention object to find the values.\n",
    "    \"\"\"\n",
    "    # d_k: 每一个头的dim\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / \\\n",
    "        math.sqrt(d_k)  # BS, 8, seqlen, seqlen\n",
    "    bs, head, seqlen = scores.size(0), scores.size(1), scores.size(2)\n",
    "\n",
    "    scores.masked_fill_(mask == 0, -1e32)\n",
    "    scores = F.softmax(scores, dim=-1)  # BS,8,seqlen,seqlen\n",
    "    # print(f\"before zero pad scores: {scores.shape}\")\n",
    "    # print(zero_pad)\n",
    "    if zero_pad:\n",
    "        pad_zero = torch.zeros(bs, head, 1, seqlen).to(device)\n",
    "        scores = torch.cat([pad_zero, scores[:, :, 1:, :]], dim=2) # 第一行score置0\n",
    "    # print(f\"after zero pad scores: {scores}\")\n",
    "    scores = dropout(scores)\n",
    "    output = torch.matmul(scores, v)\n",
    "    # import sys\n",
    "    # sys.exit()\n",
    "    return output\n",
    "\n",
    "\n",
    "class LearnablePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = 0.1 * torch.randn(max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(Dim.seq), :]  # ( 1,seq,  Feature)\n",
    "\n",
    "\n",
    "class CosinePositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = 0.1 * torch.randn(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).long()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).long() *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(Dim.seq), :]  # ( 1,seq,  Feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: unikt, emb_type: qid\n",
      "emb_c in model.parameters: True\n",
      "emb_c in model.state_dict: True\n"
     ]
    }
   ],
   "source": [
    "unikt = UNIKT()\n",
    "print(\"emb_c in model.parameters:\", any(p is unikt.emb_c for p in unikt.parameters()))\n",
    "print(\"emb_c in model.state_dict:\", 'emb_c' in unikt.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
